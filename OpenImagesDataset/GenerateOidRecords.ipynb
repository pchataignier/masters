{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import imutils\n",
    "import contextlib2\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from object_detection.dataset_tools import tf_record_creation_util\n",
    "from object_detection.utils import dataset_util, label_map_util\n",
    "#from object_detection.dataset_tools.oid_tfrecord_creation import tf_example_from_annotations_data_frame\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings and File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT = \"validation\" # [\"train\", \"test\", \"validation\"]\n",
    "LABELS_CSV = \"filteredLabels.csv\"\n",
    "LABEL_MAP_PATH = \"labelMap.pbtxt\"\n",
    "\n",
    "NUM_SHARDS = 100\n",
    "RECORDS_FILEPATH = f\"{SPLIT}/{SPLIT}.tfrecord\"\n",
    "\n",
    "ANNO_CSV = \"\"\n",
    "URLS_CSV = \"\"\n",
    "\n",
    "BALANCE_EXAMPLES = False if SPLIT == \"test\" else True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load .csv's from urls (optional)\n",
    "\n",
    "If \"ANNO_CSV\" or \"URLS_CSV\" are empty, will load the appropriate csv from:  \n",
    "https://appen.com/datasets/open-images-annotated-with-bounding-boxes/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annotations_csv = \"https://storage.googleapis.com/openimages/v6/oidv6-train-annotations-bbox.csv\"\n",
    "train_imgs_url_csv = \"https://datasets.figure-eight.com/figure_eight_datasets/open-images/train-images-boxable.csv\"\n",
    "\n",
    "val_annotations_csv = \"https://storage.googleapis.com/openimages/v5/validation-annotations-bbox.csv\"\n",
    "val_imgs_url_csv = \"https://datasets.figure-eight.com/figure_eight_datasets/open-images/validation-images.csv\"\n",
    "\n",
    "test_annotations_csv = \"https://storage.googleapis.com/openimages/v5/test-annotations-bbox.csv\"\n",
    "test_imgs_url_csv = \"https://datasets.appen.com/appen_datasets/open-images/test-images.csv\"\n",
    "\n",
    "if not ANNO_CSV:\n",
    "    if SPLIT == \"train\":\n",
    "        ANNO_CSV = train_annotations_csv\n",
    "    elif SPLIT == \"test\":\n",
    "        ANNO_CSV = test_annotations_csv\n",
    "    else:\n",
    "        ANNO_CSV = val_annotations_csv\n",
    "        \n",
    "if not URLS_CSV:\n",
    "    if SPLIT == \"train\":\n",
    "        URLS_CSV = train_imgs_url_csv\n",
    "    elif SPLIT == \"test\":\n",
    "        URLS_CSV = test_imgs_url_csv\n",
    "    else:\n",
    "        URLS_CSV = val_imgs_url_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filtered_bboxes(annotations_path, imgs_path, labels_df):\n",
    "    anno_df = pd.read_csv(annotations_path)\n",
    "    url_df = pd.read_csv(imgs_path)\n",
    "    url_df[\"ImageID\"] = url_df.image_name.apply(lambda x: x.split('.')[0])\n",
    "    url_df.drop('image_name', axis=1, inplace=True)\n",
    "\n",
    "    filtered_bboxes = anno_df[(anno_df.IsDepiction == 0) &\n",
    "                                 anno_df.LabelName.isin(labels_df.LabelName)]\n",
    "\n",
    "    filtered_bboxes = pd.merge(filtered_bboxes, labels_df, on=\"LabelName\")\n",
    "    \n",
    "    return filtered_bboxes, url_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_load_and_factors(bboxes):\n",
    "    class_load = bboxes.groupby('ClassName').agg({\"LabelName\":'count', \"ImageID\":'unique'}).reset_index()\n",
    "    class_load.rename(columns={'LabelName': 'Instances'}, inplace=True)\n",
    "\n",
    "    total_boxes = class_load.Instances.sum()\n",
    "    class_load[\"Normalized\"] = class_load.Instances.apply(lambda x: 100 * x/total_boxes)\n",
    "\n",
    "    target = class_load.Instances.max()\n",
    "    class_load[\"Factor\"] = class_load.Instances.apply(lambda x: math.ceil(target / x))\n",
    "\n",
    "    factors = pd.DataFrame(columns=[\"ImageID\", \"Factor\"])\n",
    "    for i, row in class_load.iterrows():\n",
    "        df = pd.DataFrame(row.ImageID, columns=[\"ImageID\"], dtype=str)\n",
    "        df['Factor'] = row.Factor\n",
    "        factors = pd.concat([factors, df], ignore_index=True)\n",
    "\n",
    "    factors = factors.sort_values(\"Factor\", ascending=False).drop_duplicates(\"ImageID\").reset_index(drop=True)\n",
    "    \n",
    "    return class_load, factors\n",
    "\n",
    "def plot_class_load(class_load):\n",
    "    ax = class_load.plot.bar(\"ClassName\", \"Normalized\", rot=45, figsize=(10, 5))\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"Number of boxes (%)\", fontsize=12)\n",
    "    ax.legend_.remove()\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoded_image_bytes_from_url(url):\n",
    "    img = imutils.url_to_image(url)\n",
    "    img = cv2.resize(img, (640, 480))\n",
    "    success, encoded_image = cv2.imencode('.jpg', img)\n",
    "    \n",
    "    return encoded_image.tobytes()\n",
    "\n",
    "def decode_jpg_image(encoded_jpg):\n",
    "    bytess = tf.placeholder(tf.string)\n",
    "    decode_img = tf.image.decode_image(bytess, channels=3)\n",
    "    session = tf.Session()\n",
    "    image = session.run(decode_img, feed_dict={bytess: encoded_jpg})\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label_map(label_map_path, labels):\n",
    "    with open(label_map_path, 'w+') as f:\n",
    "        for index, row in labels.iterrows():\n",
    "            line = f'item {{\\n' \\\n",
    "                            f'id: {index + 1}\\n' \\\n",
    "                            f'name: \"{row.LabelName}\"\\n' \\\n",
    "                            f'display_name: \"{row.ClassName}\"}}\\n'\n",
    "            f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix for as_matrix() deprecation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection.core import standard_fields\n",
    "from object_detection.utils import dataset_util\n",
    "\n",
    "\n",
    "def tf_example_from_annotations_data_frame(annotations_data_frame, label_map, encoded_image):\n",
    "    \"\"\"Populates a TF Example message with image annotations from a data frame.\n",
    "    Args:\n",
    "    annotations_data_frame: Data frame containing the annotations for a single\n",
    "      image.\n",
    "    label_map: String to integer label map.\n",
    "    encoded_image: The encoded image string\n",
    "    Returns:\n",
    "    The populated TF Example, if the label of at least one object is present in\n",
    "    label_map. Otherwise, returns None.\n",
    "    \"\"\"\n",
    "\n",
    "    filtered_data_frame = annotations_data_frame[annotations_data_frame.LabelName.isin(label_map)]\n",
    "    filtered_data_frame_boxes = filtered_data_frame[~filtered_data_frame.YMin.isnull()]\n",
    "    filtered_data_frame_labels = filtered_data_frame[filtered_data_frame.YMin.isnull()]\n",
    "    image_id = annotations_data_frame.ImageID.iloc[0]\n",
    "\n",
    "    feature_map = {\n",
    "      standard_fields.TfExampleFields.object_bbox_ymin:\n",
    "          dataset_util.float_list_feature(filtered_data_frame_boxes.YMin.to_numpy()),\n",
    "      standard_fields.TfExampleFields.object_bbox_xmin:\n",
    "          dataset_util.float_list_feature(filtered_data_frame_boxes.XMin.to_numpy()),\n",
    "      standard_fields.TfExampleFields.object_bbox_ymax:\n",
    "          dataset_util.float_list_feature(filtered_data_frame_boxes.YMax.to_numpy()),\n",
    "      standard_fields.TfExampleFields.object_bbox_xmax:\n",
    "          dataset_util.float_list_feature(filtered_data_frame_boxes.XMax.to_numpy()),\n",
    "      standard_fields.TfExampleFields.object_class_text:\n",
    "          dataset_util.bytes_list_feature(filtered_data_frame_boxes.LabelName.to_numpy().astype(\"bytes\")),\n",
    "      standard_fields.TfExampleFields.object_class_label:\n",
    "          dataset_util.int64_list_feature(filtered_data_frame_boxes.LabelName.map(lambda x: label_map[x]).to_numpy()),\n",
    "      standard_fields.TfExampleFields.filename:\n",
    "          dataset_util.bytes_feature('{}.jpg'.format(image_id).encode(\"utf-8\")),\n",
    "      standard_fields.TfExampleFields.source_id:\n",
    "          dataset_util.bytes_feature(image_id.encode(\"utf-8\")),\n",
    "      standard_fields.TfExampleFields.image_encoded:\n",
    "          dataset_util.bytes_feature(encoded_image),\n",
    "  }\n",
    "\n",
    "    if 'IsGroupOf' in filtered_data_frame.columns:\n",
    "        feature_map[standard_fields.TfExampleFields.object_group_of] = dataset_util.int64_list_feature(filtered_data_frame_boxes.IsGroupOf.to_numpy().astype(int))\n",
    "    if 'IsOccluded' in filtered_data_frame.columns:\n",
    "        feature_map[standard_fields.TfExampleFields.object_occluded] = dataset_util.int64_list_feature(filtered_data_frame_boxes.IsOccluded.to_numpy().astype(int))\n",
    "    if 'IsTruncated' in filtered_data_frame.columns:\n",
    "        feature_map[standard_fields.TfExampleFields.object_truncated] = dataset_util.int64_list_feature(filtered_data_frame_boxes.IsTruncated.to_numpy().astype(int))\n",
    "    if 'IsDepiction' in filtered_data_frame.columns:\n",
    "        feature_map[standard_fields.TfExampleFields.object_depiction] = dataset_util.int64_list_feature(filtered_data_frame_boxes.IsDepiction.to_numpy().astype(int))\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv(LABELS_CSV)\n",
    "bboxes, urls = get_filtered_bboxes(ANNO_CSV, URLS_CSV, labels)\n",
    "class_load, factors = get_class_load_and_factors(bboxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Class Balance and Examples to be Written\n",
    "A good chance to change the number of shards in the configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_class_load(class_load)\n",
    "t = factors.Factor.sum() if BALANCE_EXAMPLES else len(factors.ImageID.unique())\n",
    "print(f\"Total examples: {t}\\nAverage per file: {t/NUM_SHARDS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Map pbtxt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(LABEL_MAP_PATH):\n",
    "    create_label_map(LABEL_MAP_PATH, labels)\n",
    "label_dict = label_map_util.get_label_map_dict(LABEL_MAP_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint file\n",
    "Saves the image bytes and errors.  \n",
    "Used for continuing without re-downloading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_file = f\"{SPLIT}/{SPLIT}-download-summary-v2.csv\"\n",
    "\n",
    "ckpt = None\n",
    "if os.path.exists(ckpt_file):\n",
    "    ckpt = pd.read_csv(ckpt_file)\n",
    "else:\n",
    "    ckpt = pd.DataFrame([], columns=[\"ImageID\", \"Error\", \"ErrorMessage\", \"EncodedJpg\"])\n",
    "    ckpt.to_csv(ckpt_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing the Record files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total = len(bboxes.ImageID.unique())\n",
    "with contextlib2.ExitStack() as tf_record_close_stack:\n",
    "    output_tfrecords = tf_record_creation_util.open_sharded_output_tfrecords(tf_record_close_stack, \n",
    "                                                                             RECORDS_FILEPATH,\n",
    "                                                                             NUM_SHARDS)\n",
    "\n",
    "    for counter, image_data in enumerate(bboxes.groupby('ImageID')):\n",
    "        \n",
    "        image_id, image_annotations = image_data\n",
    "        #if image_id in ckpt.ImageID.unique(): continue\n",
    "        print(f\"Processing image {counter+1}/{total} at {image_id}\")\n",
    "        \n",
    "        \n",
    "        error = False\n",
    "        error_msg = \"\"\n",
    "        try:\n",
    "            if image_id in ckpt.ImageID.unique():\n",
    "                print(\"\\tAlready seen\")\n",
    "                if ckpt[ckpt.ImageID == image_id].iloc[0].Error: continue\n",
    "                else:\n",
    "                    encoded_jpg = ckpt[ckpt.ImageID == image_id].iloc[0].EncodedJpg\n",
    "            else:\n",
    "                url = urls[urls.ImageID == image_id].iloc[0].image_url\n",
    "                encoded_jpg = get_encoded_image_bytes_from_url(url)\n",
    "            \n",
    "            tf_example = tf_example_from_annotations_data_frame(image_annotations, label_dict, encoded_jpg)\n",
    "\n",
    "            if tf_example:\n",
    "                if BALANCE_EXAMPLES:\n",
    "                    factor = factors[factors.ImageID == image_id].iloc[0].Factor\n",
    "                    for i in range(factor):\n",
    "                        shard_idx = (counter + i) % NUM_SHARDS\n",
    "                        output_tfrecords[shard_idx].write(tf_example.SerializeToString())\n",
    "                else:\n",
    "                    shard_idx = counter % NUM_SHARDS\n",
    "                    output_tfrecords[shard_idx].write(tf_example.SerializeToString())\n",
    "        except Exception as ex:\n",
    "            error = True\n",
    "            error_msg = str(ex)\n",
    "            encoded_jpg = b''\n",
    "            print(error_msg)\n",
    "        \n",
    "        ckpt = ckpt.append({\"ImageID\":image_id, \"Error\":error, \"ErrorMessage\":error_msg, \"EncodedJpg\":encoded_jpg},\n",
    "                           ignore_index=True)\n",
    "        ckpt.to_csv(ckpt_file, index=False)\n",
    "        #if counter +1 >= 10:break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt[ckpt.Error][[\"ImageID\", \"ErrorMessage\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example image from each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = bboxes.groupby('LabelName').apply(pd.DataFrame.sample, n=1).reset_index(drop=True)\n",
    "df = pd.merge(df, urls, on=\"ImageID\")\n",
    "for url in df.image_url:\n",
    "    plt.figure()\n",
    "    encoded_jpg = get_encoded_image_bytes_from_url(url)\n",
    "    image = decode_jpg_image(encoded_jpg)\n",
    "    _= plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes[bboxes.IsGroupOf == 1].groupby(\"ClassName\")[\"LabelName\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_DIR = f\"../../../Mestrado/Imagens\"\n",
    "fig=ax.get_figure()\n",
    "fig.savefig(f\"{IMG_DIR}/{SPLIT}_classDistribution.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:masters-cpu]",
   "language": "python",
   "name": "conda-env-masters-cpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
